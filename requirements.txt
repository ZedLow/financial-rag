# Torch 2.4 pour être compatible avec la wheel Flash Attention
torch==2.4.0
torchvision

# On peut enfin utiliser la dernière version de Transformers !
transformers>=4.46.0
accelerate>=0.33.0
sentence-transformers
rank_bm25
gradio
pillow
numpy
pymupdf
spaces
einops
timm
qwen-vl-utils
gliner
# La wheel Flash Attention (Toujours utile pour la vitesse sur H200)
https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl